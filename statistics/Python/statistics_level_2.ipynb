{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis testing and statistical analysis - Level 2\n",
    "So far we have explored how to load, clean and summarise data in Python. In this module, we are going to learn how to test hypotheses using several methods.\n",
    "\n",
    "This is level 2. Replace '_ _ _' with code.\n",
    "\n",
    "**Brief**\n",
    "\n",
    "Did Apple Store apps receive better reviews than Google Play apps?\n",
    "\n",
    "## The challenges of the workshop are:\n",
    "\n",
    "1. Clean the two data sets\n",
    "    * Load the two datasets\n",
    "    * Pick the columns that we are going to work with \n",
    "    * Check the data types and fix them\n",
    "    * Create a column called platform whose values are “apple” or “google”\n",
    "2. Join the two data sets\n",
    "    * To do that use the function `append` with the parameter `ignore_index`\n",
    "    * Eliminate the `NaN` values\n",
    "    * Only use the apps which contain reviews\n",
    "3. Summarise the data visually and analytically (by the column `platform`)\n",
    "    * Use the function .describe()\n",
    "    * Use a boxplot\n",
    "4. Test the following hypothesis “The differences in the average ratings of apple and google users are due to chance and not due to differences in the platforms” \n",
    "    * Let’s use traditional methods: parametric and non-parametric\n",
    "    * Let’s use permutations\n",
    "\n",
    "As you are going to see, the first steps of every single data analysis are loading and cleaning the data. Today is not an exception, so that is going to be our first step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the libraries\n",
    "\n",
    "In this case we are going to import pandas, numpy, and matplotlib.pyplot. From scipy, import stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _ _ _ as pd\n",
    "import _ _ _ as np\n",
    "from _ _ _ import _ _ _\n",
    "import _ _ _ as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 1 -  Loading and cleaning data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "Load the data from the folder Stats in your Desktop, this data is from the website Kaggle. Kaggle is an extraordinary repository of data and good fun data competitions. The data from the Apple Store can be found [here](https://www.kaggle.com/ramamet4/app-store-apple-data-set-10k-apps) and the data from Google Store can be found [here](https://www.kaggle.com/lava18/google-play-store-apps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once the files are saved, we need to load them into Python using read_csv and pandas\n",
    "# Let's create a variable called google where we are going to store the path of the file\n",
    "google = _ _ _\n",
    "# Let's read the csv file into a data frame called Google\n",
    "Google = pd._ _ _ (_ _ _)\n",
    "# Let's observe the first three entries\n",
    "Google._ _ _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once the files are saved, we need to load them into Python using read_csv and pandas\n",
    "# Let's create a variable called apple where we are going to store the path of the file \n",
    "apple = _ _ _\n",
    "# Let's read the csv file into a data frame called Apple\n",
    "Apple = _ _ _\n",
    "# Let's observe the first three entries\n",
    "_ _ _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the documentation of both datasets, the most adequate columns to answer the brief are:\n",
    "\n",
    "1. For Google:\n",
    "    * `Category` # Do we need this?\n",
    "    * `Rating`\n",
    "    * `Reviews`\n",
    "    * `Price` (maybe)\n",
    "2. For Apple:    \n",
    "    * `prime_genre` # Do we need this?\n",
    "    * `user_rating` \n",
    "    * `rating_count_tot`\n",
    "    * `price` (maybe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsetting\n",
    "Let's select the columns that we want for both datasets.\n",
    "\n",
    "Overwrite the subsets in the original variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's subset the dataframe Google by only selecting the variables ['Category', 'Rating', 'Reviews', 'Price']\n",
    "_ _ _ = _ _ _ [['Category', 'Rating', 'Reviews', 'Price']]\n",
    "# Let's check the first three entries\n",
    "_ _ _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's subset the dataframe Apple by only selecting the variables ['prime_genre', 'user_rating', 'rating_count_tot', 'price']\n",
    "Apple = _ _ _\n",
    "# Let's check the first three entries\n",
    "_ _ _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking data types for both Apple and Google\n",
    "In this part let's figure out whether the variables that we selected contain errors/mistakes in the expected datatype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the atribute dtypes (which means no brackets)\n",
    "## Check out the data types of the dataframe Apple. Are the data types expected?\n",
    "Apple._ _ _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see all the data types of `Apple` are expected. What about the data types of `Google`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out the data types of the dataframe Google. Are the data types expected?\n",
    "_ _ _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the unique values of the column Price in the dataset Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function .unique to the column Price in the dataset Google\n",
    "Google[_ _ _]._ _ _()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting... There is a price called `Everyone`. That is a massive mistake. Let's check the datapoints that have the price value as `Everyone`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check what is the data point which contains the price 'Everyone' using a subset of Google.\n",
    "## Subset by the column Price that equals to `Everyone`.\n",
    "Google[_ _ _==_ _ _]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to eliminate that observation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's eliminate that point because it has the wrong information.\n",
    "## To do that, subset Google but instead of using 'is equal to' use 'is different from'.\n",
    "Google = _ _ _[_ _ _ != _ _ _ ]\n",
    "### Check again the unique values of Google\n",
    "_ _ _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the problem is that the prices contain the symbol `$`. Therefore for Python these values are still considered `str` elements and not numbers! So let's eliminate the dollar symbol and convert the column into a numeric data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a variable called nosymb.\n",
    "## That variable will take the Price column of Google and apply the str.replace function.\n",
    "### In the parameters specify to find '$' and replace with nothing ''.\n",
    "nosymb = _ _ _.str._ _ _ (_ _ _,_ _ _)\n",
    "#### Apply pd.to_numeric() to the variable nosymb, save it as the Price column in Google\n",
    "_ _ _ = _ _ _(nosymb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the data types of Google."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the atribu dtypes\n",
    "_ _ _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column `Reviews` is still an object column, we need that column to be a numeric column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function pd.to_numeric, save the result in the same column\n",
    "Google[_ _ _ ] = _ _ _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the data types of Google again\n",
    "_ _ _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New columns for `Apple` and `Google` called `platform`\n",
    "Let's create a new column called `platform` where the value for the Apple dataframe is 'apple', and for Google is 'google'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column called 'platform' and add either 'apple' or 'google' respectively\n",
    "Apple[_ _ _ ] = _ _ _\n",
    "_ _ _ [_ _ _ ] = _ _ _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing the column names to unify the two datasets\n",
    "Now we need to rename the variables of `Apple` to be the same as `Google` or vice versa.\n",
    "In this case, we are going to change the `Apple` column names to the names of `Google` columns.\n",
    "\n",
    "This is an important step to unify the two datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the nam,es of the Apple columns with the Google columns names\n",
    "Apple._ _ _  = _ _ _ .columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 2 -  Combine the two datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the two datasets into a single data frame called `df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the function append to Google \n",
    "## use Apple as the first parameter to append and set ignore_index to True\n",
    "df = _ _ _.append(_ _ _, _ _ _= _ _ _ )\n",
    "# Check 12 random points of your dataset\n",
    "_ _ _.sample(_ _ _)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see there are some `NaN` values, eliminate all the `NaN` values from the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check first the dimesions of df before dropping `NaN` values\n",
    "print(_ _ _.shape)\n",
    "# Use the function .dropna to eliminate all the NaN values, overwrite in the same dataframe\n",
    "df =  _ _ _\n",
    "# Check the dimensions of df\n",
    "print(_ _ _)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check how many apps have 0 reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset df by the column Reviews which is 0\n",
    "## Use the function .count()\n",
    "df[_ _ _ == 0]._ _ _ ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "929 apps do not have reviews, we need to eliminate these points!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminate the points that have 0 reviews by subsetting df using the expression \"different from\" !=\n",
    "df = df[_ _ _ !=_ _ _ ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 3 -  Summarise the data visually and analytically (by the column `platform`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analytical summary\n",
    "We need a summary of the column `Rating` but separated by the different platforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To summarise analytically, let's use the function `.describe` to the column `Rating`\n",
    "## after grouping by the variable `platform` \n",
    "df._ _ _(by=_ _ _)[_ _ _]._ _ _ ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual summary\n",
    "We need a summary of the column `Rating` but separated by the different platforms, let's use a boxplot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use the function boxplot on df\n",
    "## set the parameters by = 'platform' and column = ['Rating']\n",
    "df._ _ _(by=_ _ _, column = _ _ _, grid=False, rot=45, fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create the subset of the column 'Rating' by the different platforms. \n",
    "## Call the subsets 'apple' and 'google' \n",
    "apple = df[_ _ _== _ _ _][_ _ _]\n",
    "google = _ _ _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a histogram of the apple reviews distribution\n",
    "histoApple = plt.hist(_ _ _)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a histogram of the google reviews distribution\n",
    "histoGoogle = plt.hist(_ _ _)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 4 -  Test whether there are significant differences between Apple and Google reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step to test a hypothesis is to define the hypothesis that you want to test. For that reason your task here is to provide a null and an alternative hypothesis to complete the brief.\n",
    "\n",
    "H<sub>null</sub>:\n",
    "\n",
    "\n",
    "H<sub>alternative</sub>:\n",
    "\n",
    "The second step is to determine the significance level.\n",
    "\n",
    "SL: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutations \n",
    "Permutations and bootstraping are the ultimate techniques to test differences between groups. The extraordinary power lies\n",
    "in the fact that there are no previous assumptions, therefore it is very easy to apply to any problem.  \n",
    "Check out more about permutations [here](http://rasbt.github.io/mlxtend/user_guide/evaluate/permutation_test/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a variable `Permutation1` and use an analytical summary after grouping by `platform`\n",
    "## Use the function np.random.permutation\n",
    "Permutation1 = np.random.permutation(_ _ _)\n",
    "# Let's create a variable called Permutation in df with the Permutation1 values\n",
    "df[\"Permutation\"] = Permutation1\n",
    "df.groupby(by = _ _ _)[_ _ _]._ _ _ ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compare with the previous analytical summary\n",
    "df.groupby(_ _ _)[_ _ _]._ _ _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a vector with the differences - that will be the distribution of the null hypothesis \n",
    "## create an empty list called difference\n",
    "### in a for loop create a variable called permutation using the function np.random.permutation\n",
    "#### append to `difference` the mean of the permutation subset that corresponds to 'apple' minus\n",
    "##### the mean of the permutation subset that corresponds to 'google'\n",
    "difference = list()\n",
    "for _ _ _ in range(_ _ _):\n",
    "    permutation = np.random._ _ _ (_ _ _)\n",
    "    difference.append(_ _ _ - np.mean(_ _ _))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the distribution of the null hypothesis\n",
    "## Call the plot 'histo'\n",
    "### Use the function 'plt.hist'\n",
    "histo = _ _ _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what was the observed difference between the averages\n",
    "obs_difference = np.mean(_ _ _) - np.mean(_ _ _)\n",
    "## Transform that number into the absolute value\n",
    "obs_difference = _ _ _(_ _ _)\n",
    "obs_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many simulated permutations have equal or extreme value compared to the observed difference?\n",
    "extreme1 = 0\n",
    "for differences in difference: \n",
    "    if _ _ _ > _ _ _:\n",
    "        _ _ _ = _ _ _ + 1\n",
    "extreme1\n",
    "extreme2 = 0\n",
    "for _ _ _ in _ _ _: \n",
    "    if _ _ _ < -_ _ _:\n",
    "        extreme2 = _ _ _ + 1\n",
    "extreme2\n",
    "\n",
    "print(\"p-value is:\", extreme1+extreme2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other statistical tests - OPTIONAL\n",
    "Now that the hypotheses are defined, and also the significance level, we are going to try to reject or accept the null hypothesis. [Student's t-test](https://en.wikipedia.org/wiki/Student%27s_t-test#Independent_two-sample_t-test) helps to evaluate differences between two independent groups. Run a Student's t-test to compare the differences between Apple and Google reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the function stats.ttest_ind\n",
    "## in the parameters use the 'Rating' column of a subset for apple and google platform\n",
    "t_test = _ _ _(apple, google)\n",
    "# Check the test\n",
    "t_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can you conclude? Maybe the following code will help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if t_test.pvalue <= 0.05:\n",
    "    print('The p-value was:', t_test.pvalue,\n",
    "          'The observed differences are very unlikely to be due to chance, we reject the null hypothesis')\n",
    "else:\n",
    "    print('The p-value was:', t_test.pvalue,\n",
    "          'The observed differences are likely to be due to chance, we accept the null hypothesis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student's t-test has very strong assumptions that we probably overlooked. Could you write down the assumptions of the Student's t-test? You can find them [here](https://en.wikipedia.org/wiki/Student%27s_t-test#Independent_two-sample_t-test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see Student's t-test assumes that the variances between groups are equal. This is not the case (check the analytical summary), we can still use Welch’s t-test using the same function that we used before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the function stats.ttest_ind\n",
    "## in the parameters use the 'Rating' column of a subset for apple and google platform\n",
    "### the parameter equal_var set to False\n",
    "welch = _ _ _(_ _ _,\n",
    "                    _ _ _, \n",
    "                    equal_var= _ _ _)\n",
    "if welch.pvalue <= 0.05:\n",
    "    print('The p-value was:', welch.pvalue,\n",
    "          'The observed differences are very unlikely to be due to chance, we reject the null hypothesis')\n",
    "else:\n",
    "    print('The p-value was:', welch.pvalue,\n",
    "          'The observed differences are likely to be due to chance, we accept the null hypothesis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both tests (Student's t-test and Welch’s t-test) assume that the data is normally distributed, we can test that using the function stats.normaltest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the normal distribution of the apple reviews using the function stats.normaltest\n",
    "## save the result in a variable called apple_normal\n",
    "apple_normal = stats.normaltest(_ _ _)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the normal distribution of the google reviews using the function stats.normaltest\n",
    "## save the result in a variable called google_normal\n",
    "google_normal = _ _ _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The null hypothesis of the normality test is that the data is normally distributed, given that the p-values are zero we can conclude that the data is not normally distributed. You can also asses that using a histogram to see the distribution of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are non-parametric tests whose assumptions are more relaxed, the equivalent non-parametric version of a Student's t-test is Mann-Whitney U Test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function stats.mannwhitneyu to test the differences between Apple and Google\n",
    "mann_whitney = _ _ _\n",
    "mann_whitney\n",
    "if mann_whitney.pvalue <= 0.05:\n",
    "    print('The p-value was:', mann_whitney.pvalue,\n",
    "          'The observed differences are very unlikely to be due to chance, we reject the null hypothesis')\n",
    "else:\n",
    "    print('The p-value was:', mann_whitney.pvalue,\n",
    "          'The observed differences are likely to be due to chance, we accept the null hypothesis')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What can you conclude?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type your conclusion: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra challenge\n",
    "Imagine that the observed difference was 0.022, calculate the p-value for this case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
