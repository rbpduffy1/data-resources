{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time series analysis workshop \n",
    "\n",
    "\n",
    "## Guided notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the guided notebook for the time series analysis workshop. You will find the code for the visualisations and analysis we will carry out in class in the completed notebook. Feel free to follow along or review your answers after the workshop.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we will be using can be downloaded [here](https://fred.stlouisfed.org/series/DEUPROINDMISMEI) from the FRED database, the Federal Bank of St. Louis's economic database. You can download the data as a `.csv` file by clicking on the `download` button in the top right corner. \n",
    "\n",
    "This dataset shows monthly values of seasonally-adjusted total industrial production in Germany between January 1960, and October 2020. The data each month is expressed as a percentage of the value of industrial production in Germany in 2015.\n",
    "\n",
    "Once you download the dataset, make sure to save it in the same directory as this notebook so that you can read in the data without errors.\n",
    "\n",
    "**Important**: We will use this data to fit and forecast an ARIMA model. ARIMA is particularly well suited for data with little or no seasonality, such as seasonally adjusted series. We will discuss this further in the online practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal today is to underestand the properties of our time series, and to perform the necessary transformations needed to produce a simple forecast using an ARIMA model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agenda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we will:\n",
    "\n",
    "* plot the time series\n",
    "\n",
    "\n",
    "* perform a decomposition\n",
    "\n",
    "\n",
    "* test for stationarity\n",
    "\n",
    "\n",
    "* perform differencing\n",
    "\n",
    "\n",
    "* discuss and implement an `ARIMA(p,d,q)` model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary steps "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the steps we will follow for our analysis. \n",
    "\n",
    "1. Load the data \n",
    "\n",
    "2. Explore and clean\n",
    "\n",
    "3. Decompose the series and choose the right type of decomposition\n",
    "\n",
    "4. Apply `log transformation` to the series if you choose a multiplicative decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember: log transformation serves the purpose of making sure that the variance of our series is constant over time. This makes prediction easier. This will change the interpretation of the units of our series, so we will need to transform it back when we produce forecasts to interpret. We do this by performing the inverse operation: `exponential`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling steps: prepare the data for an ARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data seems to be particularly well suited for an ARIMA model, given that ARIMA requires a series to be **univariate**: only one variable, industrial production, is observed over time. Remember that we want time series data to be **evenly spaced**: to be observed at regular time intervals. This is true for this series, because that is the way this data is collected, and you can check it yourself once we import it. \n",
    "\n",
    "ARIMA also requires the data to be **stationary**, so we will run tests to assess whether that is the case, and if it is not we will apply differencing to make it so. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Plot the ACF/PACF and look for evidence of trends and/or seasonality\n",
    "\n",
    "2. Run the ADF and KPSS tests - most likely our series will not be stationary!\n",
    "\n",
    "3. Perform differencing \n",
    "\n",
    "4. Run the ADF and KPSS tests again\n",
    "\n",
    "5. Plot the ACF/PACF again to assess whether there are any remaining patterns\n",
    "\n",
    "6. Run the ADF and KPSS tests again \n",
    "\n",
    "7. Repeat steps 3-6 until the data is stationary\n",
    "\n",
    "8. Plot the ACF and PACF plots of the **stationary series**\n",
    "\n",
    "9. Select a value for `p` and `q` from the PACF and ACF plots respectively. `d` will be the number of times you differenced the series to make it stationary\n",
    "\n",
    "10. Split the data into train and test set\n",
    "\n",
    "10. Run `ARIMA(p,d,q)` using `ARIMA()` from the `statsmodels` package. **Note**: you will need to input your series **before differencing** as an input here, as this function performs the differencing for you. \n",
    "\n",
    "11. Check residuals for autocorrelation\n",
    "\n",
    "12. Produce forecasts and compare to the test set\n",
    "\n",
    "13. Transform the forecasts back from log to level "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose #we will need this \n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "import pmdarima as pm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can download the `csv` file from this [link](https://fred.stlouisfed.org/series/DEUPROINDMISMEI). Make sure you save it in the same directory as this notebook. Feel free to rename it to something more intuitive to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the .csv file from the FRED website and load it using a pandas in-built method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore and clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the head of the df. What is the frequency of our data? What is the first date available?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the tail of the df - the last day was September 1st 2020\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape of the df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore datatypes and count of values in each column\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the date column to datetime datatype\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index the data by date\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the column we are interested in to something more humanly legible\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop date from axis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The series goes back in time to the '60s, so we are working with quite a large dataset. Given the nature and composition of industrial production has changed significantly since the 1960's, we can limit our datasets to observations occurring after 1990. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep observations indexed with year=1990 or higher\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the series\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can test your intuition visually recognising multiplicative/additive decomposition series [here](https://kourentzes.com/forecasting/2014/11/09/additive-and-multiplicative-seasonality/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce both additive and multiplicative decompositions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the two decompositions against the original data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two plots compare the deterministic parts of the series (trend and seasonal components combined) produced with a multiplicative and additive model (red and green series respectively), to the actual time series (grey series). \n",
    "\n",
    "Sometimes it is visually easy to detect which decomposition model suits our data best, in this case it is not, so we will need to compare decomposition residuals. The model with the highest residuals is the worse performing one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare residuals from the two decompositions using the .resid method\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this plot, there is not much difference when it comes to the average value of the series, but it does seem that the spread of values in the additive decomposition is much higher (look at the y axis of both plots), so we will pick a **multiplicative decomposition**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run seasonal_decompose model=\"multiplicative\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot just the trend component(.trend) of the series\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot just the residual component(.resid) of the series\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot just the seasonal component(.seasonal) of the series\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we decide a multiplicative decomposition is best for our series it is generally a good idea to apply a `log transformation` to it. By doing so, we effectively transform the series into an additive decomposition series of log values. \n",
    "\n",
    "This is true because, as we saw in the introduction the deterministic part of a series `Y` in context of a multiplicative decomposition will be obtained like this: `T`x`S`, where T is the trend component, and S the seasonal one. If we apply a log transformation to this product, we get find that the `log(Y) = log(T) + log(S)`. If you are interested you can read more about logarithm algebra [here](https://courses.lumenlearning.com/boundless-algebra/chapter/working-with-logarithms/#:~:text=produce%20that%20number.-,The%20logarithm%20of%20a%20product%20is%20the%20sum%20of%20the,the%20difference%20of%20the%20logarithms).\n",
    "\n",
    "We need to do this because multiplicative series generally have **increasing variance** as the seasonal fluctuations increase in time. Instead, we need our data to have constant variance, so that we can make better predictions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log transform the series using np.log() and assign to the old variable\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling: preparing data for ARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the ACF and PACF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the acf using plot_acf()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the pacf using plot_pacf()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly observe the presence of a trend, while it is unclear that there is any seasonality. However, the presence of a trend tells us that the series is **not stationary**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stationarity: ADF test\n",
    "\n",
    "The ADF test for stationarity.\n",
    "\n",
    "H<sub>o</sub>: there is a unit root\n",
    "\n",
    "H<sub>1</sub>: there is no unit root, the series is stationary\n",
    "    \n",
    "significance level (alpha): 0.05\n",
    "\n",
    "This means that if you manage to reject the null hypothesis, `p-value < alpha`, your series is stationary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stationarity - adf test, run using adfuller() and set autolag='AIC'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the p-value of the test to 0.05. The p-value is the second output of the test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stationarity: the KPSS test\n",
    "\n",
    "KPSS test for stationarity:\n",
    "\n",
    "H<sub>o</sub>: there is no unit root, the series is stationary\n",
    "\n",
    "H<sub>1</sub>: there is a unit root\n",
    "\n",
    "This means that if you manage to reject the null hypothesis, `p-value < alpha`, your series is NOT stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the kpss test using the kpss() function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the p-value and compare to 0.05. Once again p-value is the second output of the test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differencing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that we need our series to be stationary in order to apply an ARIMA model to produce forecasts. The most common way to do that is through \"differencing\". We will subtract consecutive observations in order to remove trend and any seasonality in the series: we assume that increases in trend for consecutive periods are roughly constant, so differencing tends to be effective at removing trends.\n",
    "\n",
    "In this workshop we will discussing the case when a series has a trend, but not strong seasonality (as we have seen from the ACF/PACF plots in this industrial production data).\n",
    "\n",
    "**Important: differencing once makes a series stationary when there is a linear trend. Differencing twice makes a series stationary when there is a quadratic trend.**\n",
    "\n",
    "In the online practice we will see that simple differencing is not always enough to remove both trend and seasonality - in that case we will resort to \"seasonal differencing\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# differencing -1st order to eliminate the trend \n",
    "# drop na's resulting from differencing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's visualise the 1st differenced series\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run ADF and KPSS tests again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run adf test again and compare p-value to alpha\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run kpss test again\n",
    "# compare p-value to alpha\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot acf of the differenced series to check for any trend/seasonality left\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement ARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing ARIMA requires a set of assumptions to hold:\n",
    "\n",
    "- the data should be **stationary**, in other words its properties should not vary over time. This means that its mean and variance should be constant (homoskedastic residuals), which is why we had to apply differencing to make our series stationary. \n",
    "\n",
    "- the series should be **univariate**, that is be composed of only one variable. There are no other predictors (e.g. in an extension to ARIMA, SARIMAX, you will see that there are exceptions to this rule)\n",
    "\n",
    "We verified that our series is stationary when differenced once, and that we only have one variable, `industrial production` in this dataset.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ARIMA is composed of three hyperparameters, which we need to input when running the `ARIMA()` function**:\n",
    "\n",
    "- `p`: number of autoregressive terms (AR order)\n",
    "- `d`: number of nonseasonal differences (differencing order) \n",
    "- `q`: number of moving-average terms (MA order)\n",
    "\n",
    "**Important**: `ARIMA()` in `statsmodels` will perform the differencing for us, so we need to do it manually beforehand to understand what level of differencing is required for stationarity, in order to specify the `d` parameter when running the model. \n",
    "\n",
    "*However, your input series will be the non-differenced one as the function will do the differencing for you, you just need to specify how many differences are necessary for stationarity.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`p` and `q` are chosen by looking at the PACF and ACF plots respectively. They are chosen by looking at the lag after which the autocorrelations become statistically insignificant. \n",
    "\n",
    "**Important**: we will be looking at the PACF/ACF plots of the **stationary series**, in other words, the differenced one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's have a look at how all this works in practice**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the ACF/PACF of the stationary series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to plot the ACF and PACF plots of the stationary series, so that we can choose a value for the `p` and `q` parameters from the PACF and ACF plots respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot acf of the differenced series\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at this ACF plot to choose the best `q` value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot pacf of the differenced series\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at this ACF plot to choose the best `p` value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having looked at ACF, PACF plots and having carried out differencing to achieve stationarity we can set our initial parameters to:\n",
    "\n",
    "- `p`: 2 (PACF plot)\n",
    "- `d`: 1 (as we had to difference the series once to make it stationary)\n",
    "- `q`: 2 (ACF plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce a train test split\n",
    "\n",
    "#split into train and test set - suggested 80-20 split, but you can choose your own split\n",
    "\n",
    "\n",
    "# Check size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the ARIMA(p,d,q) model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we found our model parameters, let's run the first model, fit it and analyse it. Notice that we are using our original log transformed series and not the differenced one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run and fit our first model, print model summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot our fit\n",
    "# dynamic = False: If dynamic is False, then the in-sample lagged values are used for prediction.\n",
    "# If `dynamic` is True, then in-sample forecasts are used in place of lagged dependent variables. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important part of analysing how good a model is relies on checking residuals. In general, we want them to:\n",
    "\n",
    "- behave like white noise, and not show signs of patterns\n",
    "- not have autocorrelation\n",
    "- have a mean of 0, and have a symmetric distribution around the mean\n",
    "\n",
    "If these conditions are not met, there is likely some pattern in our data that our model is not capturing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check residuals of the model fit\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forecast - model 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our model, let's use it to produce in-sample forecasts: forecast the values of the test set, which we will then compare to the actual values to evaluate performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce forecasts and confidence intervals using the model we fit above\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the forecasts and upper/lower bounds of the confidence intervals to pd.Series()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the different series: forecast series and test set values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the series back to level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have produced forecasts, let's revert the series back to level (reverse the log transformation). We can reverse a log transformation by applying its inverse operation: the exponential. The series will look exactly the same, only the units on the y axis will change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply np.exp() to the series before plotting the forecasts\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional stretch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have run an ARIMA model, but we have not discussed how to select the best \"version\" of this model, or more formally, the best \"specification\". We will discuss this further in the online practice, but we do this using the `AIC` criteria. `AIC` stands for `Akaike Information Criteria`.\n",
    "\n",
    "We ideally want to select the version of the model with the lowest AIC possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do this automatically using techniques such as `gridsearch`, or in-built functions such as `autoarima`, which was a famous R function. Let's try the second: we will find an alternative model to the one above using autoarima, and compare the AIC we obtain with the previous one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run a model using pm.auto_arima()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run and fit the new model \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the AIC value you obtain now with the previous model's. What do you conclude?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the online practice we will discuss ways to find the optimal model, including automated techniques such as grid search. We will also discuss model evaluation techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
